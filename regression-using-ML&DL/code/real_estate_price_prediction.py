# -*- coding: utf-8 -*-
"""real-estate-price-prediction.ipynb

Automatically generated by Colaboratory.

"""

# all imports here
import os
import joblib
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib
import matplotlib.pyplot as plt
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV
import tensorflow as tf
from tensorflow.keras.models import save_model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.optimizers import Adam
from scikeras.wrappers import KerasRegressor
from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score
from sklearn.metrics import classification_report,confusion_matrix

# base_path = os.getcwd()
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath("real_estate_price_prediction.py")))

data_path = os.path.join(ROOT_DIR, "data")
model_path = os.path.join(ROOT_DIR, "model")

if not os.path.exists(model_path):
    os.mkdir(model_path)

class DataLoader:
    def __init__(self, data_path):
        self.data_path = data_path

    def load_data(self):
        # Load data from the specified path
        return pd.read_csv(self.data_path)

    def clean_data(self, data):
        # Perform data cleaning tasks
        # Dropping unnecessary rows
        # Where there is no bathroom or no bedroom!
        data.drop(data[(data['bedrooms']==0) | (data['baths']==0)].index, inplace=True)
        # Where # bathrooms is more than # bedrooms + dine, dinning.
        data.drop(data[data["baths"] > data["bedrooms"] + 2].index, inplace=True)
        # Where property type is other than House and Flats.
        unnecessary_property_types = ['Lower Portion', 'Penthouse', 'Upper Portion','Farm House', 'Room']
        data.drop(data[data["property_type"].isin(unnecessary_property_types)].index, inplace=True)
        # Where city is Karachi
        excluded_cities = ['Lahore', 'Islamabad', 'Faisalabad', 'Rawalpindi']
        data.drop(data[data["city"].isin(excluded_cities)].index, inplace=True)
        # Where location is of the following
        locations = ['Clifton', 'DHA Defence', 'Gadap Town', 'Gulistan-e-Jauhar', 'Gulshan-e-Iqbal', 'Gulshan-e-Iqbal Town',
             'Malir Cantonment', 'North Nazimabad', 'Scheme 33', 'DHA City Karachi', 'Faisal Cantonment', 'Federal B Area',
            'Gulshan-e-Maymar', 'KDA Scheme 1', 'Korangi', 'Naya Nazimabad', 'Nazimababd', 'North Karachi', 'PECHS',
             'Surjani Town']
        data.drop(data[~data["location"].isin(locations)].index, inplace=True)
        excess_columns = ['property_id', 'location_id', 'page_url', 'province_name', 'locality',
                  'latitude', 'longitude', 'area', 'area_marla','purpose', 'date_added',
                  'day', 'agency', 'agent']
        # We do consider the most important features for price prediction and thus ignore other features in the dataset.
        data = data.drop(excess_columns, axis=1)
        # Year and month fields should be qualitative and not quantitative, so converting them into string from int datatype.
        # Note: object by default
        data[['year', 'month']] = data[['year', 'month']].astype('str')
        # Getting subset
        data_subset = data[(data['year'] == '2019') & (data['month'] == '7')]
        # Resetting the dataframe on the basis of indices and not any column/field
        data_subset = data_subset.reset_index()
        data_subset = data_subset.drop("index",axis=1)
        cleaned_data = data_subset  # Placeholder for data cleaning operations
        return cleaned_data

class DataPreprocessor:
    def __init__(self, data):
        self.data = data

    def preprocess_data(self, data):
        # Perform data preprocessing tasks
        data['price_per_sqft'] = data['price'] / data['area_sqft']

        data['location'] = data['location'].apply(lambda x: x.strip())
        # Each bedroom should have above 300 square feet area
        data.drop(data[data['area_sqft'] / data['bedrooms'] < 300].index, inplace = True)
        preprocessed_data = self.data  # Placeholder for data preprocessing operations
        # Price per sqft needs to be in normal distribution
        preprocessed_data = self.remove_pps_outliers()
        # Removing the outliers for price per square feet such that as the number of bedrooms increase , price per square feet for the property also increases.
        preprocessed_data = self.remove_bhk_outliers()
        preprocessed_data = preprocessed_data[preprocessed_data["price_per_sqft"] > 3000]
        preprocessed_data = preprocessed_data.drop(['price_per_sqft'], axis=1)
        return preprocessed_data

    def remove_pps_outliers(self):
        # Implementation of remove_pps_outliers function
        data_out = pd.DataFrame()
        for key,subdf in self.data.groupby('location'):
            m = np.mean(subdf['price_per_sqft'])
            std = np.std(subdf['price_per_sqft'])
            reduced_data = subdf[(subdf['price_per_sqft'] > (m-std)) & (subdf['price_per_sqft'] <= (m+std))]
            data_out = pd.concat([data_out,reduced_data], ignore_index=True)
        return data_out

    def remove_bhk_outliers(self):
        # Implementation of remove_bhk_outliers function
        exclude_indices = np.array([])
        for location, location_df in self.data.groupby("location"):
            bhk_stats = {}
            for bedroom, bedroom_df in location_df.groupby("bedrooms"):
                bhk_stats[bedroom] = {
                    'mean' : np.mean(bedroom_df["price_per_sqft"]),
                    'std' : np.std(bedroom_df["price_per_sqft"]),
                    'count': bedroom_df.shape[0]
                }
            for bedroom, bedroom_df in location_df.groupby("bedrooms"):
                stats = bhk_stats.get(bedroom - 1)
                if stats and stats['count']>5:
                  exclude_indices = np.append(exclude_indices, bedroom_df[bedroom_df['price_per_sqft'] < (stats['mean'])].index.values)
                data_out = self.data.drop(exclude_indices, axis="index")
        return data_out

class LinearRegressionModel:
    def __init__(self):
        self.model = LinearRegression()

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict(self, X_test):
        return self.model.predict(X_test)

class DecisionTreeRegressionModel:
    def __init__(self):
        self.model = DecisionTreeRegressor()

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict(self, X_test):
        return self.model.predict(X_test)

class NeuralNetworkModel:
    def __init__(self):
        self.model = Sequential([
            Dense(32, activation='relu', input_shape=(X_train.shape[1],)),
            Dense(64, activation='relu'),
            Dense(1)
        ])
        self.model.compile(optimizer='adam', loss='mse')

    def train(self, X_train, y_train, epochs=100, batch_size=32):
        self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)

    def predict(self, X_test):
        return self.model.predict(X_test)

    def save(self, filepath):
        self.model.save(self.model, filepath)

class PropertyPricePredictor:
    def __init__(self, data_path):
        self.data_loader = DataLoader(data_path)
        self.data_preprocessor = None
        self.model = None

    def load_and_preprocess_data(self):
        data = self.data_loader.load_data()
        cleaned_data = self.data_loader.clean_data(data)
        self.data_preprocessor = DataPreprocessor(cleaned_data)
        preprocessed_data = self.data_preprocessor.preprocess_data(cleaned_data)
        preprocessed_data.to_csv(os.path.join(ROOT_DIR, 'data/preprocessed_data.csv'), encoding='utf-8')

        location_dummies = pd.get_dummies(preprocessed_data['location'])
        property_type_dummies = pd.get_dummies(preprocessed_data['property_type'])
        preprocessed_data = pd.concat([preprocessed_data, property_type_dummies, location_dummies], axis="columns")
        preprocessed_data = preprocessed_data.drop(["location", "property_type", "year", "month", "price_bin", "city"], axis=1)
        X = preprocessed_data.drop('price', axis=1)
        y = preprocessed_data['price']
        return X, y

    def train_model(self, model_type, X_train, y_train):
        if model_type == 'linear_regression':
            self.model = LinearRegressionModel()
        elif model_type == 'decision_tree':
            self.model = DecisionTreeRegressionModel()
        elif model_type == 'neural_network':
            self.model = NeuralNetworkModel()

        self.model.train(X_train, y_train)

    def evaluate_model(self, X_test, y_test):
        predictions = self.model.predict(X_test)
        mse = mean_squared_error(y_test, predictions)
        mae = mean_absolute_error(y_test,predictions)
        rmse = np.sqrt(mean_squared_error(y_test,predictions))
        variance_score = explained_variance_score(y_test,predictions)
        return mse, mae, rmse, variance_score

data_path = os.path.join(data_path, "Property_with_Feature_Engineering.csv")
predictor = PropertyPricePredictor(data_path)
X,y = predictor.load_and_preprocess_data()
# location_dummies = pd.get_dummies(preprocessed_data['location'])
# property_type_dummies = pd.get_dummies(preprocessed_data['property_type'])
# preprocessed_data = pd.concat([preprocessed_data, property_type_dummies, location_dummies], axis="columns")
# preprocessed_data = preprocessed_data.drop(["location", "property_type", "year", "month", "price_bin", "city"], axis=1)
# X = preprocessed_data.drop('price', axis=1)
# y = preprocessed_data['price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = predictor.train_model('linear_regression', X_train, y_train)
mse, mae, rmse, variance_score = predictor.evaluate_model(X_test, y_test)
print('------Linear Regression-------')
print('MAE: ',mae)
print('MSE: ',mse)
print('RMSE: ',rmse)
print('Variance Regression Score: ',variance_score)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = predictor.train_model('decision_tree', X_train, y_train)
mse, mae, rmse, variance_score = predictor.evaluate_model(X_test, y_test)
print('------Decision Tree Regression-------')
print('MAE: ',mae)
print('MSE: ',mse)
print('RMSE: ',rmse)
print('Variance Regression Score: ',variance_score)

predictor.train_model('neural_network', X_train, y_train)
mse, mae, rmse, variance_score = predictor.evaluate_model(X_test, y_test)
print('------Neural Network-------')
print('MAE: ',mae)
print('MSE: ',mse)
print('RMSE: ',rmse)
print('Variance Regression Score: ',variance_score)
# save model and architecture to single file
model_object = predictor.model
model_object.model.save(os.path.join(model_path, "model.h5"))
print("Saved model to disk")

loaded_model = tf.keras.models.load_model(os.path.join(model_path, "model.h5"))
converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)
tflite_model = converter.convert()
with open(os.path.join(model_path, "model.tflite"), "wb") as f:
    f.write(tflite_model)

print("TFLite model saved successfully.")

          
